{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ls ../input/dont-overfit-ii","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/dont-overfit-ii/train.csv\")\ntest = pd.read_csv(\"../input/dont-overfit-ii/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n* Right off the bat, we are looking at only 250 entries, with this little data of course we'll overfit.\n* Overfitting is the result of on overcomplicated model; hence, we'll deploy some sort of feature selection to simplify the model complexity and combat overfitting in this manner. Also, we'll try to perform data augmentation if possible to get a bit more samples. More data will help with overfitting.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum() # doent seem to have any missing values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().any().any() # no missing values.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(24, 24))\n\nfor i, feature in enumerate(list(train.columns)[2:27]):\n    plt.subplot(5, 5, i + 1)\n    plt.hist(train[feature])\n    plt.title(f'feature name:{feature}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above plots, looks like it's safe to assume all 302 cols/features are normally distributed.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nsns.set_style('whitegrid')\nsns.countplot(x='target',data=train, palette='RdBu_r')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Imbalance\nFrom the Histogram above, we have data imbalance. Apply SMOTE later?","metadata":{}},{"cell_type":"code","source":"train.corr()['target'].plot(kind='bar', style='k--', label='Series', grid=True, figsize=(20, 4)) \ncorr = train.corr()['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_df = corr.to_frame()\n# selecting only corr() that are greater than 0.2\ncorr_df[corr_df['target'] > 0.2] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature 18 and 128 seemed to be highly correlated with the target label.","metadata":{}},{"cell_type":"markdown","source":"## Baseline Model buildling","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,classification_report,roc_auc_score,roc_curve,auc\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\n\nX_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)\nn_fold = 20\nfolds = model_selection.StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\nrepeated_folds = model_selection.RepeatedStratifiedKFold(n_splits=20, n_repeats=20, random_state=42)\n\nfrom sklearn.model_selection import train_test_split\ntrain_x,valid_x,train_y,valid_y = train_test_split(X_train, y_train, random_state = 96, stratify=y_train)\n\n# do not also scale the X_test else you'll leak the scaling info to your test/hold-out set\nscaler = preprocessing.StandardScaler()\ntrain_x = scaler.fit_transform(train_x)\nvalid_x = scaler.transform(valid_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model train and eval function\ndef model_train_eval(algorithm,dtrain_X,dtrain_Y,dtest_X,dtest_Y,cols=None):\n    \n    algorithm.fit(dtrain_X, dtrain_Y)\n    predictions = algorithm.predict(dtest_X)\n    print (algorithm)\n    # embed()\n    print (\"ROC-AUC score : \", roc_auc_score(dtest_Y, predictions))\n    print (\"classification report :\\n\",classification_report(predictions,dtest_Y))\n    \n    prediction_probabilities = algorithm.predict_proba(dtest_X)[:,1]\n    fpr , tpr , thresholds   = roc_curve(dtest_Y,prediction_probabilities)\n    return roc_auc_score(dtest_Y, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A lot of people are using logreg currently, let's try\nmodel = LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\nmodel_train_eval(model,train_x,train_y,valid_x,valid_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ndef plot_training_curves(X_train, y_train, model):\n    train_sizes, train_scores, test_scores = learning_curve(estimator=model,\n                                               X=X_train,\n                                               y=y_train,\n                                               train_sizes=np.linspace(0.1, 1.0, 10),\n                                               cv=10,\n                                               n_jobs=1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    train_mean\n    plt.plot(train_sizes, train_mean,\n             color='blue', marker='o',\n             markersize=5, label='Training accuracy')\n\n    plt.fill_between(train_sizes,\n                     train_mean + train_std,\n                     train_mean - train_std,\n                     alpha=0.15, color='blue')\n\n    plt.plot(train_sizes, test_mean,\n             color='green', linestyle='--',\n             marker='s', markersize=5,\n             label='Validation accuracy')\n\n    plt.fill_between(train_sizes,\n                     test_mean + test_std,\n                     test_mean - test_std,\n                     alpha=0.15, color='green')\n\n    plt.grid()\n    plt.xlabel('Number of training examples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.4, 0.9])\n    plt.tight_layout()\n    # plt.savefig('images/06_05.png', dpi=300)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_training_curves(X_train, y_train, model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like pretty big gap - still Overfitting.","metadata":{}},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_submission_file(model, filename=\"submission.csv\"):\n    holdout_data = test.drop(['id'], axis=1)\n    predictions = model.predict(holdout_data)\n    \n    holdout_ids = test[\"id\"]\n    submission_df = {\"id\": holdout_ids,\n                 \"target\": predictions}\n    submission = pd.DataFrame(submission_df)\n\n    submission.to_csv(os.path.join('/kaggle/working',filename),index=False)\n    return submission\n\nsave_submission_file(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}